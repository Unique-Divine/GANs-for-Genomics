{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PyTorch\n",
    "import torch\n",
    "\n",
    "# standard DS stack\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import pandas as pd\n",
    "# embed static images in the ipynb\n",
    "%matplotlib inline \n",
    "\n",
    "# neural network package\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# !pip install statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTargets():\n",
    "    \"\"\"Retrives the target matrix from \"targets.csv\". \n",
    "    \n",
    "    The mice were scored on a test and grouped into 3 categories: GT, IR, and ST. \n",
    "    GT was the worst and ST was the best. These groups have been integer encoded.\n",
    "\n",
    "    Returns:\n",
    "        Y (np.ndarray): Phenotype values to be predicted by ML model. \n",
    "        names (np.ndarray): The names of the rats.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\"targets.csv\")\n",
    "\n",
    "    # Check if targets.csv contains the same IDs as the feature matrix\n",
    "    targetRatIDs = df.loc[(df[\"Vendor\"] == \"Charles River\")][[\"RatID\", \"Phenotype\"]].values\n",
    "    miceIDs = np.array(data.columns)[1:].astype(int)\n",
    "    assert len((a:=set(targetRatIDs[:, 0])).intersection((b:=set(miceIDs)))) == 1780\n",
    "\n",
    "    # Remove uncommon elements\n",
    "    for number in a.difference(b):\n",
    "        targetRatIDs[:, 0] = np.where((targetRatIDs[:, 0] == number), None, targetRatIDs[:, 0])    \n",
    "    targetRatIDs = pd.DataFrame(targetRatIDs, columns=[\"RatID\", \"Phenotype\"]).dropna()\n",
    "    assert targetRatIDs.shape[0] == 1780\n",
    "\n",
    "    targetRatIDs = targetRatIDs.set_index(\"RatID\").sort_index()\n",
    "    miceIDs.sort()\n",
    "    assert np.all(targetRatIDs.index.values == miceIDs.astype(int))\n",
    "\n",
    "    targetRatIDs = targetRatIDs.astype(str)\n",
    "    targetRatIDs[\"Phenotype\"].value_counts()\n",
    "\n",
    "    for i, pt in enumerate(targetRatIDs.Phenotype.values):\n",
    "        if 'GT' in pt:\n",
    "            targetRatIDs.Phenotype.iloc[i] = '0'\n",
    "        if 'IR' in pt:\n",
    "            targetRatIDs.Phenotype.iloc[i] = '1'\n",
    "        if 'ST' in pt:\n",
    "            targetRatIDs.Phenotype.iloc[i] = '2'\n",
    "\n",
    "    assert len(targetRatIDs.Phenotype.value_counts()) == 3\n",
    "    \n",
    "    Y = targetRatIDs.Phenotype.values.astype(int).reshape(-1, 1)\n",
    "    names = np.array(list(targetRatIDs.index))\n",
    "\n",
    "    assert X.shape[0] == Y.shape[0]\n",
    "\n",
    "    return Y, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCriterion(X, Y, get_coefs=True, test_fn=False) -> np.ndarray:\n",
    "    \"\"\" Get the feature selection criterion, SVM classifier coefficients. \n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray, 2D): feature matrix\n",
    "        Y (np.ndarray, 2D): target matrix\n",
    "        get_coefs (bool, optional):  Defaults to True.\n",
    "        test_fn (bool, optional): Checks whether the function works correctly using \n",
    "            a randomly generated target matrix, Y_synth. Defaults to False. \n",
    "    Returns:\n",
    "        coefs (np.ndarray, 1D)\n",
    "    \"\"\"\n",
    "    assert X.shape[0] == Y.shape[0], \"X and Y have different numbers of samples\"\n",
    "    assert Y.shape[1] == 1, \"Y needs to be a column vector\"\n",
    "    \n",
    "    if test_fn:\n",
    "        # simulated target matrix, Y\n",
    "        rng = np.random.RandomState(7)\n",
    "        Y_synth = rng.randint(0,3, X.shape[0]).reshape(-1,1)\n",
    "        \n",
    "    coefs, ps = [], []\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # for each column of X\n",
    "    for row in X.T:\n",
    "        x = row.reshape(-1, 1)\n",
    "        x = scaler.fit_transform(x, Y)\n",
    "        if get_coefs:\n",
    "            # Calculate classification coefficients\n",
    "            model = SGDClassifier(loss='hinge') # SVM classifier\n",
    "            model.fit(x, Y)\n",
    "            y_pred = model.predict(x)\n",
    "            coefs.append(model.coef_[0,0])\n",
    "\n",
    "        else:\n",
    "            # Calculate p-values from logit model\n",
    "            # sm_model = sm.Logit(Y, sm.add_constant(x)).fit(disp=0)\n",
    "            sm_model = sm.MNLogit(Y, sm.add_constant(x)).fit(disp=0)\n",
    "            ps.append(sm_model.pvalues[1])\n",
    "    \n",
    "    if get_coefs:\n",
    "        return (coefs:= np.array(coefs))\n",
    "    else:\n",
    "        return (ps:= np.array(ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def varAnalysis(X, verbose=False):\n",
    "    V = np.var(X, axis=0)\n",
    "    V_avg, V_std = np.mean(V), np.std(V)\n",
    "    V_max, V_min = np.max(V), np.min(V)\n",
    "    \n",
    "    if verbose == True:\n",
    "        print(f\"V_avg: {np.mean(V):.3f}\")    \n",
    "        print(f\"V_std: {np.std(V):.3f}\")\n",
    "        print(f\"V_max: {np.max(V):.3f}\")\n",
    "        print(f\"V_min: {np.min(V):.3f}\")\n",
    "    \n",
    "    return V_avg, V_std, V_max, V_min\n",
    "\n",
    "def plotV_info(V_info):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        V_info (dict[np.ndarray]): [description]\n",
    "    \"\"\"\n",
    "    global fig, ax\n",
    "    figscale, defaultSize = 2, np.array([8, 6])\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2 ,figsize=figscale*defaultSize)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.4, left = 0.1, right = 0.7, bottom = 0.1, top = 0.9) \n",
    "    \n",
    "    ax[0,0].hist(V_info[\"avg\"])\n",
    "    ax[0,0].set(xlabel=\"V_avg\", ylabel=\"batches\", title=\"Avg(Var)\")\n",
    "    \n",
    "    ax[1,0].hist(V_info[\"std\"])\n",
    "    ax[1,0].set(xlabel=\"V_std\", ylabel=\"batches\", title=\"Std(Var)\")\n",
    "\n",
    "    ax[0,1].hist(V_info[\"max\"])\n",
    "    ax[0,1].set(xlabel=\"V_max\", ylabel=\"batches\", title=\"Max(Var)\")\n",
    "\n",
    "    ax[1,1].hist(V_info[\"min\"])\n",
    "    ax[1,1].set(xlabel=\"V_min\", ylabel=\"batches\", title=\"Min(Var)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## `main()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0.\tTime: 0 min, 0.38 s.\tSamples per second:0.00\n",
      "Batch: 1.\tTime: 0 min, 26.17 s.\tSamples per second:76.42\n",
      "Batch: 2.\tTime: 0 min, 51.81 s.\tSamples per second:77.21\n",
      "Batch: 3.\tTime: 1 min, 16.65 s.\tSamples per second:78.27\n",
      "Batch: 4.\tTime: 1 min, 43.69 s.\tSamples per second:77.15\n",
      "Batch: 5.\tTime: 2 min, 9.10 s.\tSamples per second:77.46\n",
      "Batch: 6.\tTime: 2 min, 36.14 s.\tSamples per second:76.86\n",
      "Batch: 7.\tTime: 3 min, 1.60 s.\tSamples per second:77.09\n",
      "Batch: 8.\tTime: 3 min, 28.11 s.\tSamples per second:76.88\n",
      "Batch: 9.\tTime: 3 min, 55.49 s.\tSamples per second:76.44\n",
      "Batch: 10.\tTime: 4 min, 24.38 s.\tSamples per second:75.65\n",
      "Batch: 11.\tTime: 4 min, 50.00 s.\tSamples per second:75.86\n",
      "Batch: 12.\tTime: 5 min, 15.29 s.\tSamples per second:76.12\n",
      "Batch: 13.\tTime: 5 min, 41.94 s.\tSamples per second:76.04\n",
      "Batch: 14.\tTime: 6 min, 10.72 s.\tSamples per second:75.53\n",
      "Batch: 15.\tTime: 6 min, 38.33 s.\tSamples per second:75.31\n",
      "Batch: 16.\tTime: 7 min, 5.45 s.\tSamples per second:75.21\n",
      "Batch: 17.\tTime: 7 min, 33.13 s.\tSamples per second:75.03\n",
      "Batch: 18.\tTime: 8 min, 0.37 s.\tSamples per second:74.94\n",
      "Batch: 19.\tTime: 8 min, 28.38 s.\tSamples per second:74.75\n",
      "Batch: 20.\tTime: 8 min, 56.28 s.\tSamples per second:74.59\n",
      "Batch: 21.\tTime: 9 min, 23.26 s.\tSamples per second:74.57\n",
      "Batch: 22.\tTime: 9 min, 51.46 s.\tSamples per second:74.39\n",
      "Batch: 23.\tTime: 10 min, 18.39 s.\tSamples per second:74.39\n",
      "Batch: 24.\tTime: 10 min, 45.72 s.\tSamples per second:74.34\n",
      "Batch: 25.\tTime: 11 min, 12.94 s.\tSamples per second:74.30\n",
      "Batch: 26.\tTime: 11 min, 38.76 s.\tSamples per second:74.42\n",
      "Batch: 27.\tTime: 12 min, 5.77 s.\tSamples per second:74.40\n",
      "Batch: 28.\tTime: 12 min, 34.23 s.\tSamples per second:74.25\n",
      "Batch: 29.\tTime: 12 min, 59.82 s.\tSamples per second:74.38\n",
      "Batch: 30.\tTime: 13 min, 24.72 s.\tSamples per second:74.56\n",
      "Batch: 31.\tTime: 13 min, 53.97 s.\tSamples per second:74.34\n",
      "Batch: 32.\tTime: 14 min, 23.77 s.\tSamples per second:74.09\n",
      "Batch: 33.\tTime: 14 min, 56.32 s.\tSamples per second:73.63\n",
      "Batch: 34.\tTime: 15 min, 25.57 s.\tSamples per second:73.47\n",
      "Batch: 35.\tTime: 15 min, 52.03 s.\tSamples per second:73.53\n",
      "Batch: 36.\tTime: 16 min, 18.48 s.\tSamples per second:73.58\n",
      "Batch: 37.\tTime: 16 min, 43.92 s.\tSamples per second:73.71\n",
      "Batch: 38.\tTime: 17 min, 11.16 s.\tSamples per second:73.70\n",
      "Batch: 39.\tTime: 17 min, 39.31 s.\tSamples per second:73.63\n",
      "Batch: 40.\tTime: 18 min, 7.00 s.\tSamples per second:73.60\n",
      "Batch: 41.\tTime: 18 min, 35.03 s.\tSamples per second:73.54\n",
      "Batch: 42.\tTime: 18 min, 59.29 s.\tSamples per second:73.73\n",
      "Batch: 43.\tTime: 19 min, 23.19 s.\tSamples per second:73.93\n",
      "Batch: 44.\tTime: 19 min, 51.97 s.\tSamples per second:73.83\n",
      "Batch: 45.\tTime: 20 min, 21.23 s.\tSamples per second:73.70\n",
      "Batch: 46.\tTime: 20 min, 49.52 s.\tSamples per second:73.63\n",
      "Batch: 47.\tTime: 21 min, 17.60 s.\tSamples per second:73.58\n",
      "Batch: 48.\tTime: 21 min, 42.53 s.\tSamples per second:73.70\n",
      "Batch: 49.\tTime: 22 min, 6.82 s.\tSamples per second:73.86\n",
      "Batch: 50.\tTime: 22 min, 30.94 s.\tSamples per second:74.02\n",
      "Batch: 51.\tTime: 22 min, 55.26 s.\tSamples per second:74.17\n",
      "Batch: 52.\tTime: 23 min, 19.71 s.\tSamples per second:74.30\n",
      "Batch: 53.\tTime: 23 min, 44.69 s.\tSamples per second:74.40\n",
      "Batch: 54.\tTime: 24 min, 8.81 s.\tSamples per second:74.54\n",
      "Batch: 55.\tTime: 24 min, 33.09 s.\tSamples per second:74.67\n",
      "Batch: 56.\tTime: 24 min, 56.92 s.\tSamples per second:74.82\n",
      "Batch: 57.\tTime: 25 min, 21.33 s.\tSamples per second:74.93\n",
      "Batch: 58.\tTime: 25 min, 48.52 s.\tSamples per second:74.91\n",
      "Batch: 59.\tTime: 26 min, 19.77 s.\tSamples per second:74.69\n",
      "Batch: 60.\tTime: 26 min, 46.41 s.\tSamples per second:74.70\n",
      "Batch: 61.\tTime: 27 min, 11.71 s.\tSamples per second:74.77\n",
      "Batch: 62.\tTime: 27 min, 36.71 s.\tSamples per second:74.85\n",
      "Batch: 63.\tTime: 28 min, 2.28 s.\tSamples per second:74.90\n",
      "Batch: 64.\tTime: 28 min, 27.35 s.\tSamples per second:74.97\n",
      "Batch: 65.\tTime: 28 min, 52.41 s.\tSamples per second:75.04\n",
      "Batch: 66.\tTime: 29 min, 16.90 s.\tSamples per second:75.13\n",
      "Batch: 67.\tTime: 29 min, 41.64 s.\tSamples per second:75.21\n",
      "Batch: 68.\tTime: 30 min, 7.06 s.\tSamples per second:75.26\n",
      "Batch: 69.\tTime: 30 min, 32.22 s.\tSamples per second:75.32\n",
      "Batch: 70.\tTime: 31 min, 0.24 s.\tSamples per second:75.26\n",
      "Batch: 71.\tTime: 31 min, 31.60 s.\tSamples per second:75.07\n",
      "Batch: 72.\tTime: 32 min, 2.25 s.\tSamples per second:74.91\n",
      "Batch: 73.\tTime: 32 min, 32.82 s.\tSamples per second:74.76\n",
      "Batch: 74.\tTime: 33 min, 4.21 s.\tSamples per second:74.59\n",
      "Batch: 75.\tTime: 33 min, 35.96 s.\tSamples per second:74.41\n",
      "Batch: 76.\tTime: 34 min, 6.21 s.\tSamples per second:74.28\n",
      "Batch: 77.\tTime: 34 min, 37.52 s.\tSamples per second:74.13\n",
      "Batch: 78.\tTime: 35 min, 8.58 s.\tSamples per second:73.98\n",
      "Batch: 79.\tTime: 35 min, 41.32 s.\tSamples per second:73.79\n",
      "Batch: 80.\tTime: 36 min, 12.34 s.\tSamples per second:73.65\n",
      "Batch: 81.\tTime: 36 min, 41.59 s.\tSamples per second:73.58\n",
      "Batch: 82.\tTime: 37 min, 12.99 s.\tSamples per second:73.44\n",
      "Batch: 83.\tTime: 37 min, 43.72 s.\tSamples per second:73.33\n",
      "Batch: 84.\tTime: 38 min, 14.71 s.\tSamples per second:73.21\n",
      "Batch: 85.\tTime: 38 min, 45.53 s.\tSamples per second:73.10\n",
      "Batch: 86.\tTime: 39 min, 15.89 s.\tSamples per second:73.01\n",
      "Batch: 87.\tTime: 39 min, 46.37 s.\tSamples per second:72.91\n",
      "Batch: 88.\tTime: 40 min, 16.79 s.\tSamples per second:72.82\n",
      "Batch: 89.\tTime: 40 min, 46.53 s.\tSamples per second:72.76\n",
      "Batch: 90.\tTime: 41 min, 11.72 s.\tSamples per second:72.82\n",
      "Batch: 91.\tTime: 41 min, 37.17 s.\tSamples per second:72.88\n",
      "Batch: 92.\tTime: 42 min, 1.81 s.\tSamples per second:72.96\n",
      "Batch: 93.\tTime: 42 min, 27.22 s.\tSamples per second:73.02\n",
      "Batch: 94.\tTime: 42 min, 52.43 s.\tSamples per second:73.08\n",
      "Batch: 95.\tTime: 43 min, 17.71 s.\tSamples per second:73.14\n",
      "Batch: 96.\tTime: 43 min, 42.42 s.\tSamples per second:73.21\n",
      "Batch: 97.\tTime: 44 min, 7.16 s.\tSamples per second:73.29\n",
      "Batch: 98.\tTime: 44 min, 31.98 s.\tSamples per second:73.35\n",
      "Batch: 99.\tTime: 44 min, 56.97 s.\tSamples per second:73.42\n",
      "Batch: 100.\tTime: 45 min, 22.18 s.\tSamples per second:73.47\n",
      "Batch: 101.\tTime: 45 min, 46.90 s.\tSamples per second:73.54\n",
      "Batch: 102.\tTime: 46 min, 12.17 s.\tSamples per second:73.59\n",
      "Batch: 103.\tTime: 46 min, 36.92 s.\tSamples per second:73.65\n",
      "Batch: 104.\tTime: 47 min, 2.01 s.\tSamples per second:73.71\n",
      "Batch: 105.\tTime: 47 min, 27.56 s.\tSamples per second:73.75\n",
      "Batch: 106.\tTime: 47 min, 52.87 s.\tSamples per second:73.79\n",
      "Batch: 107.\tTime: 48 min, 17.95 s.\tSamples per second:73.85\n",
      "108\n"
     ]
    }
   ],
   "source": [
    "def main(plot_vars=False):\n",
    "    # There are about 220,000 features, so we can loop <= 110 times.\n",
    "    csvBatchSize = 2000\n",
    "    maxIteration = 112\n",
    "    \n",
    "    V_info = {}\n",
    "    V_info[\"avg\"] = np.empty(maxIteration + 1) \n",
    "    V_info[\"std\"] = np.empty(maxIteration + 1) \n",
    "    V_info[\"max\"] = np.empty(maxIteration + 1) \n",
    "    V_info[\"min\"] = np.empty(maxIteration + 1) \n",
    "    \n",
    "    global coefs_list\n",
    "    coefs_list = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for csvBatch_idx, csvBatch in enumerate(pd.read_csv(\"gtTypes_C.csv\", chunksize=csvBatchSize)):\n",
    "        current_time = time.time() - start_time\n",
    "        minutes = int(current_time / 60)\n",
    "        seconds = current_time % 60\n",
    "        print(f\"Batch: {csvBatch_idx}.\\tTime: {minutes} min, {seconds:.2f} s.\"\n",
    "             + f\"\\tSamples per second: {(csvBatchSize * csvBatch_idx) / current_time:.2f}\")\n",
    "\n",
    "        data = csvBatch\n",
    "        X = data.values[:, 1:].astype(float).T\n",
    "        \n",
    "        if csvBatch_idx == 0:\n",
    "            global Y, rat_names\n",
    "            Y, rat_names = getTargets()\n",
    "        \n",
    "        # Dynamically plot variance distributions\n",
    "        if plot_vars:\n",
    "            varAnalysisInfo = varAnalysis(X=X)\n",
    "            V_info[\"avg\"][csvBatch_idx], V_info[\"std\"][csvBatch_idx] = varAnalysisInfo[:2]\n",
    "            V_info[\"max\"][csvBatch_idx], V_info[\"min\"][csvBatch_idx] = varAnalysisInfo[2:]\n",
    "            \n",
    "            V_info_sofar = {} \n",
    "            for key in V_info:\n",
    "                V_info_sofar[key] = V_info[key][:csvBatch_idx + 1]\n",
    "            clear_output(wait=True)\n",
    "            print(f\"----------\\ncsvBatch: {csvBatch_idx}\")\n",
    "            plotV_info(V_info_sofar)\n",
    "        \n",
    "        # Store feature selection coefficients\n",
    "        coefs_list.append(getCriterion(X, Y))\n",
    "        \n",
    "        if csvBatch_idx == maxIteration:\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "        print(len(coefs))\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## post-`main()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients have already been saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(214309,)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def saveCoefs(coefs_list):\n",
    "    \"\"\" Saves the absolue value of the classification coefficients from \n",
    "    a linear SVM (feature importances). These are the weights given to each \n",
    "    feature. \n",
    "    \n",
    "    Args:\n",
    "        coefs (list[np.ndarray]): A list of the coefficients calculated\n",
    "            during batch processing. \n",
    "    Returns:\n",
    "        coefficients (np.ndarray): The saved coefficients. \n",
    "    \"\"\"\n",
    "    coefficients = np.abs(np.concatenate(coefs_list))\n",
    "\n",
    "    try:\n",
    "        coefs_exist = pd.read_csv(\"coefficients_C.csv\")\n",
    "    except:\n",
    "        coefs_exist = None\n",
    "\n",
    "    if coefs_exist is None:\n",
    "        pd.Series(coefficients).to_csv(\"coefficients_C.csv\", index=False)\n",
    "    else:\n",
    "        print(\"Coefficients have already been saved.\")\n",
    "        \n",
    "def getCoefs(group=\"C\", coefs_list=coefs_list) -> np.ndarray:\n",
    "    if group == \"C\":\n",
    "        file_name = \"coefficients_C.csv\"\n",
    "    elif group == \"H\":\n",
    "        file_name = \"coefficients_H.csv\"\n",
    "    elif group == \"both\":\n",
    "        file_name = \"coefficients.csv\"\n",
    "        \n",
    "    try:\n",
    "        coefs = pd.read_csv(file_name, index=False)\n",
    "        print(coefs.head())\n",
    "        print(coefs.shape)\n",
    "    except:\n",
    "        if isinstance(coefs_list, list):\n",
    "            coefs = np.abs(np.concatenate(coefs_list))\n",
    "        elif isinstance(coefs_list, np.ndarray):\n",
    "            coefs = np.abs(coefs_list)\n",
    "    return coefs\n",
    "\n",
    "saveCoefficients(coefs_list)\n",
    "getCoefs().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([3.7413, 2.7492, 2.6017, 2.5109, 2.4051, 2.3763, 2.3555, 2.3083, 2.3033,\n",
       "        2.2994, 2.2455, 2.2430, 2.2154, 2.1862, 2.1660, 2.1640, 2.1305, 2.1213,\n",
       "        2.0880, 2.0518, 2.0398, 2.0272, 2.0200, 2.0058, 1.9962, 1.9654, 1.9577,\n",
       "        1.9571, 1.9564, 1.9509, 1.9446, 1.9411, 1.9308, 1.9294, 1.9190, 1.9151,\n",
       "        1.9151, 1.8926, 1.8797, 1.8609, 1.8426, 1.8423, 1.8292, 1.8135, 1.8119,\n",
       "        1.8098, 1.8081, 1.8034, 1.8028, 1.8026, 1.8002, 1.7916, 1.7899, 1.7769,\n",
       "        1.7750, 1.7710, 1.7695, 1.7612, 1.7560, 1.7558, 1.7504, 1.7504, 1.7504,\n",
       "        1.7488, 1.7440, 1.7383, 1.7366, 1.7307, 1.7166, 1.7166, 1.7140, 1.7035,\n",
       "        1.6867, 1.6860, 1.6848, 1.6814, 1.6776, 1.6755, 1.6725, 1.6600, 1.6561,\n",
       "        1.6559, 1.6536, 1.6521, 1.6453, 1.6453, 1.6439, 1.6425, 1.6417, 1.6417,\n",
       "        1.6413, 1.6412, 1.6406, 1.6395, 1.6366, 1.6365, 1.6283, 1.6266, 1.6249,\n",
       "        1.6249]),\n",
       "indices=tensor([171609, 145098, 171605,  82128,  63961,  47752, 136937, 135840,  73473,\n",
       "        156677, 200137, 209291, 185504,   1453, 122686, 116840,  16609, 106349,\n",
       "         20103, 145298,  72681,  48757,  20708, 169147, 118535,  26932, 114818,\n",
       "        173377,  38469, 204622,   1024, 144257, 131417, 171584, 157985,  66860,\n",
       "         50207, 101268, 106412,  78449, 137406, 128796,  72678, 208701, 130732,\n",
       "        204889,  20793, 108460, 157955,   9002,  20963,   7556,  65444,  84702,\n",
       "        111459,  66129,   1409, 200229,  64345,  96677, 191544,  36078,  14336,\n",
       "         49088, 140513, 198263,  12900, 121251,  76040,  58075, 127756,  59708,\n",
       "        187535, 164867, 115086, 151572,  12891, 209081,   1005,  37038, 167782,\n",
       "         35778, 111928, 127323,  56300, 111407,  65425,  76498, 121113, 138729,\n",
       "         59409, 152223,  50307, 116456, 132239, 146441,  27664,  50312, 138850,\n",
       "        131863]))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(torch.Tensor(getCoefs()), k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many features were used in the NN paper?\n",
    "    # SNPs - between 350 and 20,000\n",
    "    # sample size - between 10,000 and 65,000\n",
    "# TODO get X for the smallest dataset, let's do 100 features\n",
    "# TODO Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getX_r(k, coefs, X, indices=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        k (int): The number of SNPs (features).\n",
    "        coefs (np.ndarray, 1D): Coefficients to sort by\n",
    "    Returns:\n",
    "        X_r (array-like): The reduced feature matrix.  \n",
    "    \"\"\"\n",
    "    if isinstance(k, int):\n",
    "        pass\n",
    "    elif isinstance(k, float) and (np.abs(k) < 1):\n",
    "        num_features = X.shape[1]\n",
    "        k = int(k * num_features + 1) \n",
    "    topk_coefs, topk_indices = [np.array(t) for t in torch.topk(torch.Tensor(coefs), k=k)]\n",
    "    \n",
    "    if X is not None:\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X_r = X[:, topk_indices]\n",
    "        elif isinstance(X, pd.DataFrame):\n",
    "            X_r = X.iloc[:, topk_indices]\n",
    "    else:\n",
    "        raise NotImplementedError(\"TODO | Handle case when X is None.\")\n",
    "    \n",
    "    if indices:\n",
    "        return X_r, topk_indices\n",
    "    else:\n",
    "        return X_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0.\tTime: 0 min, 0.66 s.\tSNPs/second: 0.00\n",
      "Batch: 5.\tTime: 0 min, 3.81 s.\tSNPs/second: 2816.07\n",
      "Batch: 10.\tTime: 0 min, 7.46 s.\tSNPs/second: 2875.84\n",
      "Batch: 15.\tTime: 0 min, 10.76 s.\tSNPs/second: 2987.74\n",
      "Batch: 20.\tTime: 0 min, 13.85 s.\tSNPs/second: 3095.65\n",
      "Batch: 25.\tTime: 0 min, 18.69 s.\tSNPs/second: 2867.74\n",
      "Batch: 30.\tTime: 0 min, 21.93 s.\tSNPs/second: 2932.53\n",
      "Batch: 35.\tTime: 0 min, 24.97 s.\tSNPs/second: 3004.61\n",
      "Batch: 40.\tTime: 0 min, 27.95 s.\tSNPs/second: 3067.93\n",
      "Batch: 45.\tTime: 0 min, 31.01 s.\tSNPs/second: 3111.12\n",
      "Batch: 50.\tTime: 0 min, 34.04 s.\tSNPs/second: 3148.83\n",
      "Batch: 55.\tTime: 0 min, 37.07 s.\tSNPs/second: 3180.67\n",
      "Batch: 60.\tTime: 0 min, 40.17 s.\tSNPs/second: 3202.73\n",
      "Batch: 65.\tTime: 0 min, 43.32 s.\tSNPs/second: 3216.74\n",
      "Batch: 70.\tTime: 0 min, 46.43 s.\tSNPs/second: 3232.56\n",
      "Batch: 75.\tTime: 0 min, 49.49 s.\tSNPs/second: 3248.89\n",
      "Batch: 80.\tTime: 0 min, 52.54 s.\tSNPs/second: 3264.65\n",
      "Batch: 85.\tTime: 0 min, 55.62 s.\tSNPs/second: 3276.47\n",
      "Batch: 90.\tTime: 0 min, 58.68 s.\tSNPs/second: 3288.42\n",
      "Batch: 95.\tTime: 1 min, 1.71 s.\tSNPs/second: 3300.41\n"
     ]
    }
   ],
   "source": [
    "def splitX(splits, time_it=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        splits (int): The number of partitions the data will be split into. \n",
    "            Decides the batch size. \n",
    "    \"\"\"\n",
    "    Xs, SNP_names_list = [], []\n",
    "    csvBatchSize = int((getCoefs().size / splits) + 1)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for csvBatch_idx, csvBatch in enumerate(pd.read_csv(\"gtTypes_C.csv\", chunksize=csvBatchSize)):\n",
    "        data = csvBatch\n",
    "        coef_arr_idx_bounds = np.array([csvBatch_idx, csvBatch_idx + 1]) * csvBatchSize  \n",
    "        coef_arr = getCoefs()[coef_arr_idx_bounds[0]: coef_arr_idx_bounds[1]]\n",
    "        X = data.values[:, 1:].astype(float).T\n",
    "        SNP_names = data.values[:, 0].astype(str)\n",
    "        assert coef_arr.size == X.shape[1]\n",
    "        X_r, indices_r = getX_r(k = 0.1, coefs=coef_arr, X=X, indices=True)\n",
    "        assert X_r.shape[1] == indices_r.size, \\\n",
    "            \"The number of columns in X_r doesn't match the number of SNP_names.\"\n",
    "        Xs.append(X_r)\n",
    "\n",
    "        SNP_names = SNP_names[indices_r]\n",
    "        SNP_names_list.append(SNP_names)\n",
    "        \n",
    "        if time_it and (csvBatch_idx % 5 == 0):\n",
    "            current_time = time.time() - start_time\n",
    "            minutes = int(current_time / 60)\n",
    "            seconds = current_time % 60\n",
    "            print(f\"Batch: {csvBatch_idx}.\\tTime: {minutes} min, {seconds:.2f} s.\"\n",
    "                 + f\"\\tSNPs/second: {(csvBatchSize * csvBatch_idx) / current_time:.2f}\")\n",
    "\n",
    "    return Xs, SNP_names_list\n",
    "\n",
    "Xs, SNP_names_list = splitX(splits=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1780, 21491), (21491,))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack(Xs).shape, np.concatenate(SNP_names_list).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    733\n",
       "0    687\n",
       "2    360\n",
       "dtype: int64"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(Y).hist()\n",
    "pd.DataFrame(Y).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chr1.24842254</th>\n",
       "      <th>chr1.18210036</th>\n",
       "      <th>chr1.24379965</th>\n",
       "      <th>chr1.17933243</th>\n",
       "      <th>chr1.26839871</th>\n",
       "      <th>chr1.10892619</th>\n",
       "      <th>chr1.24594301</th>\n",
       "      <th>chr1.28247944</th>\n",
       "      <th>chr1.19895068</th>\n",
       "      <th>chr1.19674800</th>\n",
       "      <th>...</th>\n",
       "      <th>chr20.50225537</th>\n",
       "      <th>chr20.55553404</th>\n",
       "      <th>chr20.50196910</th>\n",
       "      <th>chr20.40913925</th>\n",
       "      <th>chr20.40915369</th>\n",
       "      <th>chr20.31341442</th>\n",
       "      <th>chr20.34574827</th>\n",
       "      <th>chr20.48317405</th>\n",
       "      <th>chr20.30814607</th>\n",
       "      <th>chr20.44643590</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1780 rows × 21491 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      chr1.24842254  chr1.18210036  chr1.24379965  chr1.17933243  \\\n",
       "0               0.0            0.0            0.0            0.0   \n",
       "1               0.0            0.0            0.0            0.0   \n",
       "2               0.0            0.0            0.0            0.0   \n",
       "3               0.0            0.0            1.0            0.0   \n",
       "4               0.0            0.0            1.0            0.0   \n",
       "...             ...            ...            ...            ...   \n",
       "1775            1.0            0.0            0.0            0.0   \n",
       "1776            2.0            0.0            0.0            0.0   \n",
       "1777            2.0            0.0            0.0            0.0   \n",
       "1778            1.0            0.0            0.0            0.0   \n",
       "1779            2.0            0.0            0.0            0.0   \n",
       "\n",
       "      chr1.26839871  chr1.10892619  chr1.24594301  chr1.28247944  \\\n",
       "0               0.0            1.0            0.0            1.0   \n",
       "1               0.0            2.0            0.0            1.0   \n",
       "2               0.0            2.0            1.0            1.0   \n",
       "3               0.0            0.0            1.0            1.0   \n",
       "4               0.0            1.0            1.0            2.0   \n",
       "...             ...            ...            ...            ...   \n",
       "1775            2.0            0.0            0.0            2.0   \n",
       "1776            1.0            2.0            0.0            2.0   \n",
       "1777            1.0            0.0            0.0            2.0   \n",
       "1778            2.0            0.0            0.0            1.0   \n",
       "1779            2.0            0.0            0.0            0.0   \n",
       "\n",
       "      chr1.19895068  chr1.19674800  ...  chr20.50225537  chr20.55553404  \\\n",
       "0               0.0            0.0  ...             0.0             0.0   \n",
       "1               2.0            0.0  ...             0.0             1.0   \n",
       "2               1.0            0.0  ...             1.0             0.0   \n",
       "3               1.0            0.0  ...             1.0             0.0   \n",
       "4               1.0            0.0  ...             0.0             0.0   \n",
       "...             ...            ...  ...             ...             ...   \n",
       "1775            2.0            0.0  ...             1.0             0.0   \n",
       "1776            0.0            0.0  ...             2.0             0.0   \n",
       "1777            1.0            0.0  ...             1.0             0.0   \n",
       "1778            0.0            0.0  ...             1.0             0.0   \n",
       "1779            1.0            0.0  ...             1.0             0.0   \n",
       "\n",
       "      chr20.50196910  chr20.40913925  chr20.40915369  chr20.31341442  \\\n",
       "0                0.0             0.0             0.0             0.0   \n",
       "1                0.0             0.0             0.0             0.0   \n",
       "2                0.0             0.0             0.0             0.0   \n",
       "3                1.0             0.0             0.0             0.0   \n",
       "4                0.0             0.0             0.0             0.0   \n",
       "...              ...             ...             ...             ...   \n",
       "1775             0.0             0.0             0.0             0.0   \n",
       "1776             1.0             0.0             0.0             0.0   \n",
       "1777             1.0             0.0             0.0             0.0   \n",
       "1778             0.0             0.0             0.0             0.0   \n",
       "1779             0.0             0.0             0.0             0.0   \n",
       "\n",
       "      chr20.34574827  chr20.48317405  chr20.30814607  chr20.44643590  \n",
       "0                1.0             1.0             0.0             0.0  \n",
       "1                0.0             0.0             0.0             0.0  \n",
       "2                0.0             0.0             0.0             0.0  \n",
       "3                0.0             0.0             0.0             1.0  \n",
       "4                1.0             0.0             1.0             0.0  \n",
       "...              ...             ...             ...             ...  \n",
       "1775             0.0             1.0             0.0             0.0  \n",
       "1776             1.0             1.0             0.0             0.0  \n",
       "1777             0.0             0.0             0.0             0.0  \n",
       "1778             0.0             0.0             0.0             0.0  \n",
       "1779             0.0             1.0             0.0             0.0  \n",
       "\n",
       "[1780 rows x 21491 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_r = pd.DataFrame(data = np.hstack(Xs), columns = np.concatenate(SNP_names_list))\n",
    "X_r "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pd.read_csv(\"gtTypes_C.csv\")\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1780, 100), (21491,))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform the analysis with SNPs selected from SelectKBest.\n",
    "\n",
    "%time \n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "selectk = SelectKBest(chi2, k=100)\n",
    "\n",
    "X_dummy = selectk.fit_transform(X_r, Y)\n",
    "X_dummy.shape, selectk.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([16.4646, 16.2303, 16.2303, 15.8967, 15.6809, 15.6047, 15.6047, 15.5528,\n",
       "        15.4021, 13.8975, 13.8662, 13.5612, 13.3569, 13.3569, 13.1151, 12.9964,\n",
       "        12.9187, 12.6300, 12.5502, 12.3182, 12.0947, 12.0738, 12.0590, 12.0514,\n",
       "        11.9404, 11.9239, 11.8714, 11.8288, 11.7775, 11.5188, 11.4483, 11.4479,\n",
       "        11.4269, 11.3765, 11.3754, 11.3754, 11.0688, 11.0688, 10.9905, 10.9341,\n",
       "        10.9100, 10.9053, 10.8866, 10.8635, 10.8580, 10.7840, 10.7840, 10.7840,\n",
       "        10.7542, 10.7375, 10.7150, 10.7121, 10.7074, 10.6794, 10.6397, 10.6397,\n",
       "        10.6325, 10.6045, 10.5636, 10.5636, 10.5633, 10.4765, 10.4699, 10.4582,\n",
       "        10.4493, 10.4434, 10.4314, 10.3176, 10.1845, 10.1309, 10.1106, 10.0951,\n",
       "        10.0496, 10.0417, 10.0224,  9.9342,  9.9319,  9.9252,  9.8783,  9.8372,\n",
       "         9.8041,  9.7947,  9.7889,  9.7600,  9.7600,  9.7355,  9.7343,  9.7343,\n",
       "         9.7257,  9.6761,  9.6577,  9.6470,  9.5917,  9.5916,  9.5811,  9.5563,\n",
       "         9.5467,  9.5334,  9.5243,  9.4958]),\n",
       "indices=tensor([10036,  3322,  3341,  3328, 15708,  1300,  1432,  1290, 10721,  3070,\n",
       "        19152,  5031,  4971,  4988, 17551,  8634,  6635,  6498, 15840,  5405,\n",
       "         9503, 19954,  6553,  8520, 14495,  2605,   905, 12432, 15675,    29,\n",
       "         5465, 10700,  3375,  7941,  7764,  7863, 14835, 14888,  5052, 17042,\n",
       "           52, 20904,  4899, 20103, 13169,  3387,  3271,  3272, 15737, 15700,\n",
       "        11352, 17826,  7794,  5567, 12314, 12336, 19377, 10170,  7066,  6946,\n",
       "         7844,  4741,  3389, 19478,  3334,  5388,  4495,  4866,  5332, 17192,\n",
       "        18473,  2214, 15044,  5144,  7798,   272,  7785,  5235,  6659,  7842,\n",
       "        16291,  4496, 16164, 12263, 12265,  5177,  2263,  2171,  9516,   391,\n",
       "        12347, 20613, 14221,  5090,  6537,  8552, 20545, 16318,   994, 19942]))"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(torch.Tensor(selectk.scores_), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([16.4646, 16.2303, 16.2303, 15.8967, 15.6809, 15.6047, 15.6047, 15.5528,\n",
       "        15.4021, 13.8975, 13.8662, 13.5612, 13.3569, 13.3569, 13.1151, 12.9964,\n",
       "        12.9187, 12.6300, 12.5502, 12.3182, 12.0947, 12.0738, 12.0590, 12.0514,\n",
       "        11.9404, 11.9239, 11.8714, 11.8288, 11.7775, 11.5188, 11.4483, 11.4479,\n",
       "        11.4269, 11.3765, 11.3754, 11.3754, 11.0688, 11.0688, 10.9905, 10.9341,\n",
       "        10.9100, 10.9053, 10.8866, 10.8635, 10.8580, 10.7840, 10.7840, 10.7840,\n",
       "        10.7542, 10.7375, 10.7150, 10.7121, 10.7074, 10.6794, 10.6397, 10.6397,\n",
       "        10.6325, 10.6045, 10.5636, 10.5636, 10.5633, 10.4765, 10.4699, 10.4582,\n",
       "        10.4493, 10.4434, 10.4314, 10.3176, 10.1845, 10.1309, 10.1106, 10.0951,\n",
       "        10.0496, 10.0417, 10.0224,  9.9342,  9.9319,  9.9252,  9.8783,  9.8372,\n",
       "         9.8041,  9.7947,  9.7889,  9.7600,  9.7600,  9.7355,  9.7343,  9.7343,\n",
       "         9.7257,  9.6761,  9.6577,  9.6470,  9.5917,  9.5916,  9.5811,  9.5563,\n",
       "         9.5467,  9.5334,  9.5243,  9.4958]),\n",
       "indices=tensor([10036,  3322,  3341,  3328, 15708,  1300,  1432,  1290, 10721,  3070,\n",
       "        19152,  5031,  4971,  4988, 17551,  8634,  6635,  6498, 15840,  5405,\n",
       "         9503, 19954,  6553,  8520, 14495,  2605,   905, 12432, 15675,    29,\n",
       "         5465, 10700,  3375,  7941,  7764,  7863, 14835, 14888,  5052, 17042,\n",
       "           52, 20904,  4899, 20103, 13169,  3387,  3271,  3272, 15737, 15700,\n",
       "        11352, 17826,  7794,  5567, 12314, 12336, 19377, 10170,  7066,  6946,\n",
       "         7844,  4741,  3389, 19478,  3334,  5388,  4495,  4866,  5332, 17192,\n",
       "        18473,  2214, 15044,  5144,  7798,   272,  7785,  5235,  6659,  7842,\n",
       "        16291,  4496, 16164, 12263, 12265,  5177,  2263,  2171,  9516,   391,\n",
       "        12347, 20613, 14221,  5090,  6537,  8552, 20545, 16318,   994, 19942]))"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(torch.Tensor(chi2_stats), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['chr7.29631577', 'chr2.145686126', 'chr2.145743050',\n",
       "       'chr2.144811756', 'chr13.46775590', 'chr1.196721088',\n",
       "       'chr1.196721085', 'chr1.196721089', 'chr7.110434734',\n",
       "       'chr2.108960762', 'chr17.22777658', 'chr3.104950986',\n",
       "       'chr3.104990952', 'chr3.104991105', 'chr15.14899164',\n",
       "       'chr6.39478341', 'chr4.119528150', 'chr4.118967275',\n",
       "       'chr13.46784040', 'chr3.143256926', 'chr6.130924586',\n",
       "       'chr18.13497419', 'chr4.119363345', 'chr6.6674844',\n",
       "       'chr11.33028547', 'chr2.43248723', 'chr1.132793881',\n",
       "       'chr9.40597849', 'chr12.49835217', 'chr1.19942050',\n",
       "       'chr3.142909318', 'chr7.110496699', 'chr2.143772703',\n",
       "       'chr5.99313779', 'chr5.96661534', 'chr5.96661537',\n",
       "       'chr11.75034553', 'chr11.75034575', 'chr3.103333129',\n",
       "       'chr14.72625273', 'chr1.19942048', 'chr20.5368892',\n",
       "       'chr3.68664801', 'chr18.26857012', 'chr9.116352923',\n",
       "       'chr2.143817940', 'chr2.143822315', 'chr2.143821170',\n",
       "       'chr13.46746378', 'chr13.46818894', 'chr8.51523137',\n",
       "       'chr15.36249080', 'chr5.97384376', 'chr3.143212486',\n",
       "       'chr9.36755184', 'chr9.36755134', 'chr17.36926663',\n",
       "       'chr7.67214102', 'chr4.171707733', 'chr4.171707641',\n",
       "       'chr5.96905834', 'chr3.68649676', 'chr2.145317036',\n",
       "       'chr17.22920424', 'chr2.144733105', 'chr3.145044482',\n",
       "       'chr3.16365216', 'chr3.68749082', 'chr3.139665185',\n",
       "       'chr14.72619764', 'chr16.2706085', 'chr2.6203968',\n",
       "       'chr11.74622783', 'chr3.104537571', 'chr5.95887867',\n",
       "       'chr1.53168329', 'chr5.97285180', 'chr3.139559088',\n",
       "       'chr4.118769841', 'chr5.97382427', 'chr13.77646381',\n",
       "       'chr3.12148776', 'chr13.78064083', 'chr9.39811771',\n",
       "       'chr9.39811755', 'chr3.124167777', 'chr2.7694369', 'chr2.7694461',\n",
       "       'chr6.131088208', 'chr1.52440739', 'chr9.36862967',\n",
       "       'chr18.84669450', 'chr10.102894506', 'chr3.103010344',\n",
       "       'chr4.119044499', 'chr6.4744373', 'chr18.84663583',\n",
       "       'chr13.77798834', 'chr1.134526662', 'chr17.88547268'], dtype='<U15')"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(SNP_names_list)[np.array(torch.topk(torch.Tensor(chi2_stats), 100)[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi2_stats, p_vals = chi2(X_r, Y)\n",
    "chi2_stats, p_vals\n",
    "X_r.values[:, np.array(torch.topk(torch.Tensor(chi2_stats), 100)[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 1., 0.],\n",
       "       [0., 0., 0., ..., 2., 2., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 1., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 1., 0.]])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References & Further Reading:\n",
    "\n",
    "- Jie Yuan1:26 PM https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/\n",
    "- Jie Yuan1:30 PM https://stackoverflow.com/questions/25122999/scikit-learn-how-to-check-coefficients-significance\n",
    "- McKelvey & Zavoina https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds_env)",
   "language": "python",
   "name": "ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
