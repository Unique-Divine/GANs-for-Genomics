{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#main()\" data-toc-modified-id=\"main()-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span><code>main()</code></a></span></li><li><span><a href=\"#post-main()\" data-toc-modified-id=\"post-main()-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>post-<code>main()</code></a></span></li><li><span><a href=\"#ML-Stuff\" data-toc-modified-id=\"ML-Stuff-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>ML Stuff</a></span></li><li><span><a href=\"#References-&amp;-Further-Reading:\" data-toc-modified-id=\"References-&amp;-Further-Reading:-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>References &amp; Further Reading:</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PyTorch\n",
    "import torch\n",
    "\n",
    "# standard DS stack\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import pandas as pd\n",
    "# embed static images in the ipynb\n",
    "%matplotlib inline \n",
    "\n",
    "# neural network package\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# !pip install statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTargets():\n",
    "    \"\"\"Retrives the target matrix from \"targets.csv\". \n",
    "    \n",
    "    The mice were scored on a test and grouped into 3 categories: GT, IR, and ST. \n",
    "    GT was the worst and ST was the best. These groups have been integer encoded.\n",
    "\n",
    "    Returns:\n",
    "        Y (np.ndarray): Phenotype values to be predicted by ML model. \n",
    "        names (np.ndarray): The names of the rats.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\"targets.csv\")\n",
    "\n",
    "    # Check if targets.csv contains the same IDs as the feature matrix\n",
    "    targetRatIDs = df.loc[(df[\"Vendor\"] == \"Charles River\")][[\"RatID\", \"Phenotype\"]].values\n",
    "    miceIDs = np.array(data.columns)[1:].astype(int)\n",
    "    assert len((a:=set(targetRatIDs[:, 0])).intersection((b:=set(miceIDs)))) == 1780\n",
    "\n",
    "    # Remove uncommon elements\n",
    "    for number in a.difference(b):\n",
    "        targetRatIDs[:, 0] = np.where((targetRatIDs[:, 0] == number), None, targetRatIDs[:, 0])    \n",
    "    targetRatIDs = pd.DataFrame(targetRatIDs, columns=[\"RatID\", \"Phenotype\"]).dropna()\n",
    "    assert targetRatIDs.shape[0] == 1780\n",
    "\n",
    "    targetRatIDs = targetRatIDs.set_index(\"RatID\").sort_index()\n",
    "    miceIDs.sort()\n",
    "    assert np.all(targetRatIDs.index.values == miceIDs.astype(int))\n",
    "\n",
    "    targetRatIDs = targetRatIDs.astype(str)\n",
    "    targetRatIDs[\"Phenotype\"].value_counts()\n",
    "\n",
    "    for i, pt in enumerate(targetRatIDs.Phenotype.values):\n",
    "        if 'GT' in pt:\n",
    "            targetRatIDs.Phenotype.iloc[i] = '0'\n",
    "        if 'IR' in pt:\n",
    "            targetRatIDs.Phenotype.iloc[i] = '1'\n",
    "        if 'ST' in pt:\n",
    "            targetRatIDs.Phenotype.iloc[i] = '2'\n",
    "\n",
    "    assert len(targetRatIDs.Phenotype.value_counts()) == 3\n",
    "    \n",
    "    Y = targetRatIDs.Phenotype.values.astype(int).reshape(-1, 1)\n",
    "    names = np.array(list(targetRatIDs.index))\n",
    "\n",
    "    assert X.shape[0] == Y.shape[0]\n",
    "\n",
    "    return Y, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCriterion(X, Y, get_coefs=True, test_fn=False) -> np.ndarray:\n",
    "    \"\"\" Get the feature selection criterion, SVM classifier coefficients. \n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray, 2D): feature matrix\n",
    "        Y (np.ndarray, 2D): target matrix\n",
    "        get_coefs (bool, optional):  Defaults to True.\n",
    "        test_fn (bool, optional): Checks whether the function works correctly using \n",
    "            a randomly generated target matrix, Y_synth. Defaults to False. \n",
    "    Returns:\n",
    "        coefs (np.ndarray, 1D)\n",
    "    \"\"\"\n",
    "    assert X.shape[0] == Y.shape[0], \"X and Y have different numbers of samples\"\n",
    "    assert Y.shape[1] == 1, \"Y needs to be a column vector\"\n",
    "    \n",
    "    if test_fn:\n",
    "        # simulated target matrix, Y\n",
    "        rng = np.random.RandomState(7)\n",
    "        Y_synth = rng.randint(0,3, X.shape[0]).reshape(-1,1)\n",
    "        \n",
    "    coefs, ps = [], []\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # for each column of X\n",
    "    for row in X.T:\n",
    "        x = row.reshape(-1, 1)\n",
    "        x = scaler.fit_transform(x, Y)\n",
    "        if get_coefs:\n",
    "            # Calculate classification coefficients\n",
    "            model = SGDClassifier(loss='hinge') # SVM classifier\n",
    "            model.fit(x, Y)\n",
    "            y_pred = model.predict(x)\n",
    "            coefs.append(model.coef_[0,0])\n",
    "\n",
    "        else:\n",
    "            # Calculate p-values from logit model\n",
    "            # sm_model = sm.Logit(Y, sm.add_constant(x)).fit(disp=0)\n",
    "            sm_model = sm.MNLogit(Y, sm.add_constant(x)).fit(disp=0)\n",
    "            ps.append(sm_model.pvalues[1])\n",
    "    \n",
    "    if get_coefs:\n",
    "        return (coefs:= np.array(coefs))\n",
    "    else:\n",
    "        return (ps:= np.array(ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def varAnalysis(X, verbose=False):\n",
    "    V = np.var(X, axis=0)\n",
    "    V_avg, V_std = np.mean(V), np.std(V)\n",
    "    V_max, V_min = np.max(V), np.min(V)\n",
    "    \n",
    "    if verbose == True:\n",
    "        print(f\"V_avg: {np.mean(V):.3f}\")    \n",
    "        print(f\"V_std: {np.std(V):.3f}\")\n",
    "        print(f\"V_max: {np.max(V):.3f}\")\n",
    "        print(f\"V_min: {np.min(V):.3f}\")\n",
    "    \n",
    "    return V_avg, V_std, V_max, V_min\n",
    "\n",
    "def plotV_info(V_info):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        V_info (dict[np.ndarray]): [description]\n",
    "    \"\"\"\n",
    "    global fig, ax\n",
    "    figscale, defaultSize = 2, np.array([8, 6])\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2 ,figsize=figscale*defaultSize)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.4, left = 0.1, right = 0.7, bottom = 0.1, top = 0.9) \n",
    "    \n",
    "    ax[0,0].hist(V_info[\"avg\"])\n",
    "    ax[0,0].set(xlabel=\"V_avg\", ylabel=\"batches\", title=\"Avg(Var)\")\n",
    "    \n",
    "    ax[1,0].hist(V_info[\"std\"])\n",
    "    ax[1,0].set(xlabel=\"V_std\", ylabel=\"batches\", title=\"Std(Var)\")\n",
    "\n",
    "    ax[0,1].hist(V_info[\"max\"])\n",
    "    ax[0,1].set(xlabel=\"V_max\", ylabel=\"batches\", title=\"Max(Var)\")\n",
    "\n",
    "    ax[1,1].hist(V_info[\"min\"])\n",
    "    ax[1,1].set(xlabel=\"V_min\", ylabel=\"batches\", title=\"Min(Var)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## `main()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0.\tTime: 0 min, 0.38 s.\tSamples per second:0.00\n",
      "Batch: 1.\tTime: 0 min, 26.17 s.\tSamples per second:76.42\n",
      "Batch: 2.\tTime: 0 min, 51.81 s.\tSamples per second:77.21\n",
      "Batch: 3.\tTime: 1 min, 16.65 s.\tSamples per second:78.27\n",
      "Batch: 4.\tTime: 1 min, 43.69 s.\tSamples per second:77.15\n",
      "Batch: 5.\tTime: 2 min, 9.10 s.\tSamples per second:77.46\n",
      "Batch: 6.\tTime: 2 min, 36.14 s.\tSamples per second:76.86\n",
      "Batch: 7.\tTime: 3 min, 1.60 s.\tSamples per second:77.09\n",
      "Batch: 8.\tTime: 3 min, 28.11 s.\tSamples per second:76.88\n",
      "Batch: 9.\tTime: 3 min, 55.49 s.\tSamples per second:76.44\n",
      "Batch: 10.\tTime: 4 min, 24.38 s.\tSamples per second:75.65\n",
      "Batch: 11.\tTime: 4 min, 50.00 s.\tSamples per second:75.86\n",
      "Batch: 12.\tTime: 5 min, 15.29 s.\tSamples per second:76.12\n",
      "Batch: 13.\tTime: 5 min, 41.94 s.\tSamples per second:76.04\n",
      "Batch: 14.\tTime: 6 min, 10.72 s.\tSamples per second:75.53\n",
      "Batch: 15.\tTime: 6 min, 38.33 s.\tSamples per second:75.31\n",
      "Batch: 16.\tTime: 7 min, 5.45 s.\tSamples per second:75.21\n",
      "Batch: 17.\tTime: 7 min, 33.13 s.\tSamples per second:75.03\n",
      "Batch: 18.\tTime: 8 min, 0.37 s.\tSamples per second:74.94\n",
      "Batch: 19.\tTime: 8 min, 28.38 s.\tSamples per second:74.75\n",
      "Batch: 20.\tTime: 8 min, 56.28 s.\tSamples per second:74.59\n",
      "Batch: 21.\tTime: 9 min, 23.26 s.\tSamples per second:74.57\n",
      "Batch: 22.\tTime: 9 min, 51.46 s.\tSamples per second:74.39\n",
      "Batch: 23.\tTime: 10 min, 18.39 s.\tSamples per second:74.39\n",
      "Batch: 24.\tTime: 10 min, 45.72 s.\tSamples per second:74.34\n",
      "Batch: 25.\tTime: 11 min, 12.94 s.\tSamples per second:74.30\n",
      "Batch: 26.\tTime: 11 min, 38.76 s.\tSamples per second:74.42\n",
      "Batch: 27.\tTime: 12 min, 5.77 s.\tSamples per second:74.40\n",
      "Batch: 28.\tTime: 12 min, 34.23 s.\tSamples per second:74.25\n",
      "Batch: 29.\tTime: 12 min, 59.82 s.\tSamples per second:74.38\n",
      "Batch: 30.\tTime: 13 min, 24.72 s.\tSamples per second:74.56\n",
      "Batch: 31.\tTime: 13 min, 53.97 s.\tSamples per second:74.34\n",
      "Batch: 32.\tTime: 14 min, 23.77 s.\tSamples per second:74.09\n",
      "Batch: 33.\tTime: 14 min, 56.32 s.\tSamples per second:73.63\n",
      "Batch: 34.\tTime: 15 min, 25.57 s.\tSamples per second:73.47\n",
      "Batch: 35.\tTime: 15 min, 52.03 s.\tSamples per second:73.53\n",
      "Batch: 36.\tTime: 16 min, 18.48 s.\tSamples per second:73.58\n",
      "Batch: 37.\tTime: 16 min, 43.92 s.\tSamples per second:73.71\n",
      "Batch: 38.\tTime: 17 min, 11.16 s.\tSamples per second:73.70\n",
      "Batch: 39.\tTime: 17 min, 39.31 s.\tSamples per second:73.63\n",
      "Batch: 40.\tTime: 18 min, 7.00 s.\tSamples per second:73.60\n",
      "Batch: 41.\tTime: 18 min, 35.03 s.\tSamples per second:73.54\n",
      "Batch: 42.\tTime: 18 min, 59.29 s.\tSamples per second:73.73\n",
      "Batch: 43.\tTime: 19 min, 23.19 s.\tSamples per second:73.93\n",
      "Batch: 44.\tTime: 19 min, 51.97 s.\tSamples per second:73.83\n",
      "Batch: 45.\tTime: 20 min, 21.23 s.\tSamples per second:73.70\n",
      "Batch: 46.\tTime: 20 min, 49.52 s.\tSamples per second:73.63\n",
      "Batch: 47.\tTime: 21 min, 17.60 s.\tSamples per second:73.58\n",
      "Batch: 48.\tTime: 21 min, 42.53 s.\tSamples per second:73.70\n",
      "Batch: 49.\tTime: 22 min, 6.82 s.\tSamples per second:73.86\n",
      "Batch: 50.\tTime: 22 min, 30.94 s.\tSamples per second:74.02\n",
      "Batch: 51.\tTime: 22 min, 55.26 s.\tSamples per second:74.17\n",
      "Batch: 52.\tTime: 23 min, 19.71 s.\tSamples per second:74.30\n",
      "Batch: 53.\tTime: 23 min, 44.69 s.\tSamples per second:74.40\n",
      "Batch: 54.\tTime: 24 min, 8.81 s.\tSamples per second:74.54\n",
      "Batch: 55.\tTime: 24 min, 33.09 s.\tSamples per second:74.67\n",
      "Batch: 56.\tTime: 24 min, 56.92 s.\tSamples per second:74.82\n",
      "Batch: 57.\tTime: 25 min, 21.33 s.\tSamples per second:74.93\n",
      "Batch: 58.\tTime: 25 min, 48.52 s.\tSamples per second:74.91\n",
      "Batch: 59.\tTime: 26 min, 19.77 s.\tSamples per second:74.69\n",
      "Batch: 60.\tTime: 26 min, 46.41 s.\tSamples per second:74.70\n",
      "Batch: 61.\tTime: 27 min, 11.71 s.\tSamples per second:74.77\n",
      "Batch: 62.\tTime: 27 min, 36.71 s.\tSamples per second:74.85\n",
      "Batch: 63.\tTime: 28 min, 2.28 s.\tSamples per second:74.90\n",
      "Batch: 64.\tTime: 28 min, 27.35 s.\tSamples per second:74.97\n",
      "Batch: 65.\tTime: 28 min, 52.41 s.\tSamples per second:75.04\n",
      "Batch: 66.\tTime: 29 min, 16.90 s.\tSamples per second:75.13\n",
      "Batch: 67.\tTime: 29 min, 41.64 s.\tSamples per second:75.21\n",
      "Batch: 68.\tTime: 30 min, 7.06 s.\tSamples per second:75.26\n",
      "Batch: 69.\tTime: 30 min, 32.22 s.\tSamples per second:75.32\n",
      "Batch: 70.\tTime: 31 min, 0.24 s.\tSamples per second:75.26\n",
      "Batch: 71.\tTime: 31 min, 31.60 s.\tSamples per second:75.07\n",
      "Batch: 72.\tTime: 32 min, 2.25 s.\tSamples per second:74.91\n",
      "Batch: 73.\tTime: 32 min, 32.82 s.\tSamples per second:74.76\n",
      "Batch: 74.\tTime: 33 min, 4.21 s.\tSamples per second:74.59\n",
      "Batch: 75.\tTime: 33 min, 35.96 s.\tSamples per second:74.41\n",
      "Batch: 76.\tTime: 34 min, 6.21 s.\tSamples per second:74.28\n",
      "Batch: 77.\tTime: 34 min, 37.52 s.\tSamples per second:74.13\n",
      "Batch: 78.\tTime: 35 min, 8.58 s.\tSamples per second:73.98\n",
      "Batch: 79.\tTime: 35 min, 41.32 s.\tSamples per second:73.79\n",
      "Batch: 80.\tTime: 36 min, 12.34 s.\tSamples per second:73.65\n",
      "Batch: 81.\tTime: 36 min, 41.59 s.\tSamples per second:73.58\n",
      "Batch: 82.\tTime: 37 min, 12.99 s.\tSamples per second:73.44\n",
      "Batch: 83.\tTime: 37 min, 43.72 s.\tSamples per second:73.33\n",
      "Batch: 84.\tTime: 38 min, 14.71 s.\tSamples per second:73.21\n",
      "Batch: 85.\tTime: 38 min, 45.53 s.\tSamples per second:73.10\n",
      "Batch: 86.\tTime: 39 min, 15.89 s.\tSamples per second:73.01\n",
      "Batch: 87.\tTime: 39 min, 46.37 s.\tSamples per second:72.91\n",
      "Batch: 88.\tTime: 40 min, 16.79 s.\tSamples per second:72.82\n",
      "Batch: 89.\tTime: 40 min, 46.53 s.\tSamples per second:72.76\n",
      "Batch: 90.\tTime: 41 min, 11.72 s.\tSamples per second:72.82\n",
      "Batch: 91.\tTime: 41 min, 37.17 s.\tSamples per second:72.88\n",
      "Batch: 92.\tTime: 42 min, 1.81 s.\tSamples per second:72.96\n",
      "Batch: 93.\tTime: 42 min, 27.22 s.\tSamples per second:73.02\n",
      "Batch: 94.\tTime: 42 min, 52.43 s.\tSamples per second:73.08\n",
      "Batch: 95.\tTime: 43 min, 17.71 s.\tSamples per second:73.14\n",
      "Batch: 96.\tTime: 43 min, 42.42 s.\tSamples per second:73.21\n",
      "Batch: 97.\tTime: 44 min, 7.16 s.\tSamples per second:73.29\n",
      "Batch: 98.\tTime: 44 min, 31.98 s.\tSamples per second:73.35\n",
      "Batch: 99.\tTime: 44 min, 56.97 s.\tSamples per second:73.42\n",
      "Batch: 100.\tTime: 45 min, 22.18 s.\tSamples per second:73.47\n",
      "Batch: 101.\tTime: 45 min, 46.90 s.\tSamples per second:73.54\n",
      "Batch: 102.\tTime: 46 min, 12.17 s.\tSamples per second:73.59\n",
      "Batch: 103.\tTime: 46 min, 36.92 s.\tSamples per second:73.65\n",
      "Batch: 104.\tTime: 47 min, 2.01 s.\tSamples per second:73.71\n",
      "Batch: 105.\tTime: 47 min, 27.56 s.\tSamples per second:73.75\n",
      "Batch: 106.\tTime: 47 min, 52.87 s.\tSamples per second:73.79\n",
      "Batch: 107.\tTime: 48 min, 17.95 s.\tSamples per second:73.85\n",
      "108\n"
     ]
    }
   ],
   "source": [
    "def main(plot_vars=False):\n",
    "    # There are about 220,000 features, so we can loop <= 110 times.\n",
    "    csvBatchSize = 2000\n",
    "    maxIteration = 112\n",
    "    \n",
    "    V_info = {}\n",
    "    V_info[\"avg\"] = np.empty(maxIteration + 1) \n",
    "    V_info[\"std\"] = np.empty(maxIteration + 1) \n",
    "    V_info[\"max\"] = np.empty(maxIteration + 1) \n",
    "    V_info[\"min\"] = np.empty(maxIteration + 1) \n",
    "    \n",
    "    global coefs_list\n",
    "    coefs_list = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for csvBatch_idx, csvBatch in enumerate(pd.read_csv(\"gtTypes_C.csv\", chunksize=csvBatchSize)):\n",
    "        current_time = time.time() - start_time\n",
    "        minutes = int(current_time / 60)\n",
    "        seconds = current_time % 60\n",
    "        print(f\"Batch: {csvBatch_idx}.\\tTime: {minutes} min, {seconds:.2f} s.\"\n",
    "             + f\"\\tSamples per second: {(csvBatchSize * csvBatch_idx) / current_time:.2f}\")\n",
    "\n",
    "        data = csvBatch\n",
    "        X = data.values[:, 1:].astype(float).T\n",
    "        \n",
    "        if csvBatch_idx == 0:\n",
    "            global Y, rat_names\n",
    "            Y, rat_names = getTargets()\n",
    "        \n",
    "        # Dynamically plot variance distributions\n",
    "        if plot_vars:\n",
    "            varAnalysisInfo = varAnalysis(X=X)\n",
    "            V_info[\"avg\"][csvBatch_idx], V_info[\"std\"][csvBatch_idx] = varAnalysisInfo[:2]\n",
    "            V_info[\"max\"][csvBatch_idx], V_info[\"min\"][csvBatch_idx] = varAnalysisInfo[2:]\n",
    "            \n",
    "            V_info_sofar = {} \n",
    "            for key in V_info:\n",
    "                V_info_sofar[key] = V_info[key][:csvBatch_idx + 1]\n",
    "            clear_output(wait=True)\n",
    "            print(f\"----------\\ncsvBatch: {csvBatch_idx}\")\n",
    "            plotV_info(V_info_sofar)\n",
    "        \n",
    "        # Store feature selection coefficients\n",
    "        coefs_list.append(getCriterion(X, Y))\n",
    "        \n",
    "        if csvBatch_idx == maxIteration:\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "        print(len(coefs))\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## post-`main()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients have already been saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(214309,)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def saveCoefs(coefs_list):\n",
    "    \"\"\" Saves the absolue value of the classification coefficients from \n",
    "    a linear SVM (feature importances). These are the weights given to each \n",
    "    feature. \n",
    "    \n",
    "    Args:\n",
    "        coefs (list[np.ndarray]): A list of the coefficients calculated\n",
    "            during batch processing. \n",
    "    Returns:\n",
    "        coefficients (np.ndarray): The saved coefficients. \n",
    "    \"\"\"\n",
    "    coefficients = np.abs(np.concatenate(coefs_list))\n",
    "\n",
    "    try:\n",
    "        coefs_exist = pd.read_csv(\"coefficients_C.csv\")\n",
    "    except:\n",
    "        coefs_exist = None\n",
    "\n",
    "    if coefs_exist is None:\n",
    "        pd.Series(coefficients).to_csv(\"coefficients_C.csv\", index=False)\n",
    "    else:\n",
    "        print(\"Coefficients have already been saved.\")\n",
    "        \n",
    "def getCoefs(group=\"C\", coefs_list=coefs_list) -> np.ndarray:\n",
    "    if group == \"C\":\n",
    "        file_name = \"coefficients_C.csv\"\n",
    "    elif group == \"H\":\n",
    "        file_name = \"coefficients_H.csv\"\n",
    "    elif group == \"both\":\n",
    "        file_name = \"coefficients.csv\"\n",
    "        \n",
    "    try:\n",
    "        coefs = pd.read_csv(file_name, index=False)\n",
    "        print(coefs.head())\n",
    "        print(coefs.shape)\n",
    "    except:\n",
    "        if isinstance(coefs_list, list):\n",
    "            coefs = np.abs(np.concatenate(coefs_list))\n",
    "        elif isinstance(coefs_list, np.ndarray):\n",
    "            coefs = np.abs(coefs_list)\n",
    "    return coefs\n",
    "\n",
    "saveCoefficients(coefs_list)\n",
    "getCoefs().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([3.7413, 2.7492, 2.6017, 2.5109, 2.4051, 2.3763, 2.3555, 2.3083, 2.3033,\n",
       "        2.2994, 2.2455, 2.2430, 2.2154, 2.1862, 2.1660, 2.1640, 2.1305, 2.1213,\n",
       "        2.0880, 2.0518, 2.0398, 2.0272, 2.0200, 2.0058, 1.9962, 1.9654, 1.9577,\n",
       "        1.9571, 1.9564, 1.9509, 1.9446, 1.9411, 1.9308, 1.9294, 1.9190, 1.9151,\n",
       "        1.9151, 1.8926, 1.8797, 1.8609, 1.8426, 1.8423, 1.8292, 1.8135, 1.8119,\n",
       "        1.8098, 1.8081, 1.8034, 1.8028, 1.8026, 1.8002, 1.7916, 1.7899, 1.7769,\n",
       "        1.7750, 1.7710, 1.7695, 1.7612, 1.7560, 1.7558, 1.7504, 1.7504, 1.7504,\n",
       "        1.7488, 1.7440, 1.7383, 1.7366, 1.7307, 1.7166, 1.7166, 1.7140, 1.7035,\n",
       "        1.6867, 1.6860, 1.6848, 1.6814, 1.6776, 1.6755, 1.6725, 1.6600, 1.6561,\n",
       "        1.6559, 1.6536, 1.6521, 1.6453, 1.6453, 1.6439, 1.6425, 1.6417, 1.6417,\n",
       "        1.6413, 1.6412, 1.6406, 1.6395, 1.6366, 1.6365, 1.6283, 1.6266, 1.6249,\n",
       "        1.6249]),\n",
       "indices=tensor([171609, 145098, 171605,  82128,  63961,  47752, 136937, 135840,  73473,\n",
       "        156677, 200137, 209291, 185504,   1453, 122686, 116840,  16609, 106349,\n",
       "         20103, 145298,  72681,  48757,  20708, 169147, 118535,  26932, 114818,\n",
       "        173377,  38469, 204622,   1024, 144257, 131417, 171584, 157985,  66860,\n",
       "         50207, 101268, 106412,  78449, 137406, 128796,  72678, 208701, 130732,\n",
       "        204889,  20793, 108460, 157955,   9002,  20963,   7556,  65444,  84702,\n",
       "        111459,  66129,   1409, 200229,  64345,  96677, 191544,  36078,  14336,\n",
       "         49088, 140513, 198263,  12900, 121251,  76040,  58075, 127756,  59708,\n",
       "        187535, 164867, 115086, 151572,  12891, 209081,   1005,  37038, 167782,\n",
       "         35778, 111928, 127323,  56300, 111407,  65425,  76498, 121113, 138729,\n",
       "         59409, 152223,  50307, 116456, 132239, 146441,  27664,  50312, 138850,\n",
       "        131863]))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(torch.Tensor(getCoefs()), k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many features were used in the NN paper?\n",
    "    # SNPs - between 350 and 20,000\n",
    "    # sample size - between 10,000 and 65,000\n",
    "# TODO get X for the smallest dataset, let's do 100 features\n",
    "# TODO Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getX_r(k, coefs, X, indices=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        k (int): The number of SNPs (features).\n",
    "        coefs (np.ndarray, 1D): Coefficients to sort by\n",
    "    Returns:\n",
    "        X_r (array-like): The reduced feature matrix.  \n",
    "    \"\"\"\n",
    "    if isinstance(k, int):\n",
    "        pass\n",
    "    elif isinstance(k, float) and (np.abs(k) < 1):\n",
    "        num_features = X.shape[1]\n",
    "        k = int(k * num_features + 1) \n",
    "    topk_coefs, topk_indices = [np.array(t) for t in torch.topk(torch.Tensor(coefs), k=k)]\n",
    "    \n",
    "    if X is not None:\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X_r = X[:, topk_indices]\n",
    "        elif isinstance(X, pd.DataFrame):\n",
    "            X_r = X.iloc[:, topk_indices]\n",
    "    else:\n",
    "        raise NotImplementedError(\"TODO | Handle case when X is None.\")\n",
    "    \n",
    "    if indices:\n",
    "        return X_r, topk_indices\n",
    "    else:\n",
    "        return X_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0.\tTime: 0 min, 0.66 s.\tSNPs/second: 0.00\n",
      "Batch: 5.\tTime: 0 min, 3.81 s.\tSNPs/second: 2816.07\n",
      "Batch: 10.\tTime: 0 min, 7.46 s.\tSNPs/second: 2875.84\n",
      "Batch: 15.\tTime: 0 min, 10.76 s.\tSNPs/second: 2987.74\n",
      "Batch: 20.\tTime: 0 min, 13.85 s.\tSNPs/second: 3095.65\n",
      "Batch: 25.\tTime: 0 min, 18.69 s.\tSNPs/second: 2867.74\n",
      "Batch: 30.\tTime: 0 min, 21.93 s.\tSNPs/second: 2932.53\n",
      "Batch: 35.\tTime: 0 min, 24.97 s.\tSNPs/second: 3004.61\n",
      "Batch: 40.\tTime: 0 min, 27.95 s.\tSNPs/second: 3067.93\n",
      "Batch: 45.\tTime: 0 min, 31.01 s.\tSNPs/second: 3111.12\n",
      "Batch: 50.\tTime: 0 min, 34.04 s.\tSNPs/second: 3148.83\n",
      "Batch: 55.\tTime: 0 min, 37.07 s.\tSNPs/second: 3180.67\n",
      "Batch: 60.\tTime: 0 min, 40.17 s.\tSNPs/second: 3202.73\n",
      "Batch: 65.\tTime: 0 min, 43.32 s.\tSNPs/second: 3216.74\n",
      "Batch: 70.\tTime: 0 min, 46.43 s.\tSNPs/second: 3232.56\n",
      "Batch: 75.\tTime: 0 min, 49.49 s.\tSNPs/second: 3248.89\n",
      "Batch: 80.\tTime: 0 min, 52.54 s.\tSNPs/second: 3264.65\n",
      "Batch: 85.\tTime: 0 min, 55.62 s.\tSNPs/second: 3276.47\n",
      "Batch: 90.\tTime: 0 min, 58.68 s.\tSNPs/second: 3288.42\n",
      "Batch: 95.\tTime: 1 min, 1.71 s.\tSNPs/second: 3300.41\n"
     ]
    }
   ],
   "source": [
    "def splitX(splits, time_it=True, return_type=\"array\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        splits (int): The number of partitions the data will be split into. \n",
    "            Decides the batch size. \n",
    "    Returns:\n",
    "        Xs (return_type)\n",
    "        SNP_names_list (return_type)\n",
    "        return_type (str): \"np.ndarray\" or \"list\".\n",
    "    \"\"\"\n",
    "    assert return_type == (\"array\" or \"list\"), \\\n",
    "        \"return_type must be an 'array' or 'list'.\"\n",
    "    \n",
    "    Xs, SNP_names_list = [], []\n",
    "    csvBatchSize = int((getCoefs().size / splits) + 1)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for csvBatch_idx, csvBatch in enumerate(\n",
    "            pd.read_csv(\"gtTypes_C.csv\", chunksize=csvBatchSize)):\n",
    "        data = csvBatch\n",
    "        coef_arr_idx_bounds = np.array([csvBatch_idx, csvBatch_idx + 1]) * csvBatchSize  \n",
    "        coef_arr = getCoefs()[coef_arr_idx_bounds[0]: coef_arr_idx_bounds[1]]\n",
    "        X = data.values[:, 1:].astype(float).T\n",
    "        SNP_names = data.values[:, 0].astype(str)\n",
    "        assert coef_arr.size == X.shape[1]\n",
    "        X_r, indices_r = getX_r(k = 0.1, coefs=coef_arr, X=X, indices=True)\n",
    "        assert X_r.shape[1] == indices_r.size, \\\n",
    "            \"The column count of X_r doesn't match the number of SNP_names.\"\n",
    "        Xs.append(X_r)\n",
    "\n",
    "        SNP_names = SNP_names[indices_r]\n",
    "        SNP_names_list.append(SNP_names)\n",
    "        \n",
    "        if time_it and (csvBatch_idx % 5 == 0):\n",
    "            current_time = time.time() - start_time\n",
    "            minutes = int(current_time / 60)\n",
    "            seconds = current_time % 60\n",
    "            print(f\"Batch: {csvBatch_idx}.\\tTime: {minutes} min, {seconds:.2f} s.\"\n",
    "                 + f\"\\tSNPs/second: {(csvBatchSize * csvBatch_idx) / current_time:.2f}\")\n",
    "    if return_type == \"array\":\n",
    "        Xs = np.hstack(Xs)\n",
    "        SNP_names = np.concatenate(SNP_names_list)\n",
    "    elif return_type == \"list\": \n",
    "        SNP_names = SNP_names_list\n",
    "    return Xs, SNP_names\n",
    "\n",
    "Xs, SNP_names = splitX(splits=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chr1.24842254</th>\n",
       "      <th>chr1.18210036</th>\n",
       "      <th>chr1.24379965</th>\n",
       "      <th>chr1.17933243</th>\n",
       "      <th>chr1.26839871</th>\n",
       "      <th>chr1.10892619</th>\n",
       "      <th>chr1.24594301</th>\n",
       "      <th>chr1.28247944</th>\n",
       "      <th>chr1.19895068</th>\n",
       "      <th>chr1.19674800</th>\n",
       "      <th>...</th>\n",
       "      <th>chr20.50225537</th>\n",
       "      <th>chr20.55553404</th>\n",
       "      <th>chr20.50196910</th>\n",
       "      <th>chr20.40913925</th>\n",
       "      <th>chr20.40915369</th>\n",
       "      <th>chr20.31341442</th>\n",
       "      <th>chr20.34574827</th>\n",
       "      <th>chr20.48317405</th>\n",
       "      <th>chr20.30814607</th>\n",
       "      <th>chr20.44643590</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1780 rows × 21491 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      chr1.24842254  chr1.18210036  chr1.24379965  chr1.17933243  \\\n",
       "0               0.0            0.0            0.0            0.0   \n",
       "1               0.0            0.0            0.0            0.0   \n",
       "2               0.0            0.0            0.0            0.0   \n",
       "3               0.0            0.0            1.0            0.0   \n",
       "4               0.0            0.0            1.0            0.0   \n",
       "...             ...            ...            ...            ...   \n",
       "1775            1.0            0.0            0.0            0.0   \n",
       "1776            2.0            0.0            0.0            0.0   \n",
       "1777            2.0            0.0            0.0            0.0   \n",
       "1778            1.0            0.0            0.0            0.0   \n",
       "1779            2.0            0.0            0.0            0.0   \n",
       "\n",
       "      chr1.26839871  chr1.10892619  chr1.24594301  chr1.28247944  \\\n",
       "0               0.0            1.0            0.0            1.0   \n",
       "1               0.0            2.0            0.0            1.0   \n",
       "2               0.0            2.0            1.0            1.0   \n",
       "3               0.0            0.0            1.0            1.0   \n",
       "4               0.0            1.0            1.0            2.0   \n",
       "...             ...            ...            ...            ...   \n",
       "1775            2.0            0.0            0.0            2.0   \n",
       "1776            1.0            2.0            0.0            2.0   \n",
       "1777            1.0            0.0            0.0            2.0   \n",
       "1778            2.0            0.0            0.0            1.0   \n",
       "1779            2.0            0.0            0.0            0.0   \n",
       "\n",
       "      chr1.19895068  chr1.19674800  ...  chr20.50225537  chr20.55553404  \\\n",
       "0               0.0            0.0  ...             0.0             0.0   \n",
       "1               2.0            0.0  ...             0.0             1.0   \n",
       "2               1.0            0.0  ...             1.0             0.0   \n",
       "3               1.0            0.0  ...             1.0             0.0   \n",
       "4               1.0            0.0  ...             0.0             0.0   \n",
       "...             ...            ...  ...             ...             ...   \n",
       "1775            2.0            0.0  ...             1.0             0.0   \n",
       "1776            0.0            0.0  ...             2.0             0.0   \n",
       "1777            1.0            0.0  ...             1.0             0.0   \n",
       "1778            0.0            0.0  ...             1.0             0.0   \n",
       "1779            1.0            0.0  ...             1.0             0.0   \n",
       "\n",
       "      chr20.50196910  chr20.40913925  chr20.40915369  chr20.31341442  \\\n",
       "0                0.0             0.0             0.0             0.0   \n",
       "1                0.0             0.0             0.0             0.0   \n",
       "2                0.0             0.0             0.0             0.0   \n",
       "3                1.0             0.0             0.0             0.0   \n",
       "4                0.0             0.0             0.0             0.0   \n",
       "...              ...             ...             ...             ...   \n",
       "1775             0.0             0.0             0.0             0.0   \n",
       "1776             1.0             0.0             0.0             0.0   \n",
       "1777             1.0             0.0             0.0             0.0   \n",
       "1778             0.0             0.0             0.0             0.0   \n",
       "1779             0.0             0.0             0.0             0.0   \n",
       "\n",
       "      chr20.34574827  chr20.48317405  chr20.30814607  chr20.44643590  \n",
       "0                1.0             1.0             0.0             0.0  \n",
       "1                0.0             0.0             0.0             0.0  \n",
       "2                0.0             0.0             0.0             0.0  \n",
       "3                0.0             0.0             0.0             1.0  \n",
       "4                1.0             0.0             1.0             0.0  \n",
       "...              ...             ...             ...             ...  \n",
       "1775             0.0             1.0             0.0             0.0  \n",
       "1776             1.0             1.0             0.0             0.0  \n",
       "1777             0.0             0.0             0.0             0.0  \n",
       "1778             0.0             0.0             0.0             0.0  \n",
       "1779             0.0             1.0             0.0             0.0  \n",
       "\n",
       "[1780 rows x 21491 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_r = pd.DataFrame(data = Xs, columns = SNP_names)\n",
    "X_r "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1780, 100), (21491,))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform the analysis with SNPs selected from SelectKBest.\n",
    "\n",
    "%time \n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "selectk = SelectKBest(chi2, k=100)\n",
    "\n",
    "X_dummy = selectk.fit_transform(X_r, Y)\n",
    "X_dummy.shape, selectk.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([16.4646, 16.2303, 16.2303, 15.8967, 15.6809, 15.6047, 15.6047, 15.5528,\n",
       "        15.4021, 13.8975, 13.8662, 13.5612, 13.3569, 13.3569, 13.1151, 12.9964,\n",
       "        12.9187, 12.6300, 12.5502, 12.3182, 12.0947, 12.0738, 12.0590, 12.0514,\n",
       "        11.9404, 11.9239, 11.8714, 11.8288, 11.7775, 11.5188, 11.4483, 11.4479,\n",
       "        11.4269, 11.3765, 11.3754, 11.3754, 11.0688, 11.0688, 10.9905, 10.9341,\n",
       "        10.9100, 10.9053, 10.8866, 10.8635, 10.8580, 10.7840, 10.7840, 10.7840,\n",
       "        10.7542, 10.7375, 10.7150, 10.7121, 10.7074, 10.6794, 10.6397, 10.6397,\n",
       "        10.6325, 10.6045, 10.5636, 10.5636, 10.5633, 10.4765, 10.4699, 10.4582,\n",
       "        10.4493, 10.4434, 10.4314, 10.3176, 10.1845, 10.1309, 10.1106, 10.0951,\n",
       "        10.0496, 10.0417, 10.0224,  9.9342,  9.9319,  9.9252,  9.8783,  9.8372,\n",
       "         9.8041,  9.7947,  9.7889,  9.7600,  9.7600,  9.7355,  9.7343,  9.7343,\n",
       "         9.7257,  9.6761,  9.6577,  9.6470,  9.5917,  9.5916,  9.5811,  9.5563,\n",
       "         9.5467,  9.5334,  9.5243,  9.4958]),\n",
       "indices=tensor([10036,  3322,  3341,  3328, 15708,  1300,  1432,  1290, 10721,  3070,\n",
       "        19152,  5031,  4971,  4988, 17551,  8634,  6635,  6498, 15840,  5405,\n",
       "         9503, 19954,  6553,  8520, 14495,  2605,   905, 12432, 15675,    29,\n",
       "         5465, 10700,  3375,  7941,  7764,  7863, 14835, 14888,  5052, 17042,\n",
       "           52, 20904,  4899, 20103, 13169,  3387,  3271,  3272, 15737, 15700,\n",
       "        11352, 17826,  7794,  5567, 12314, 12336, 19377, 10170,  7066,  6946,\n",
       "         7844,  4741,  3389, 19478,  3334,  5388,  4495,  4866,  5332, 17192,\n",
       "        18473,  2214, 15044,  5144,  7798,   272,  7785,  5235,  6659,  7842,\n",
       "        16291,  4496, 16164, 12263, 12265,  5177,  2263,  2171,  9516,   391,\n",
       "        12347, 20613, 14221,  5090,  6537,  8552, 20545, 16318,   994, 19942]))"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(torch.Tensor(selectk.scores_), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([16.4646, 16.2303, 16.2303, 15.8967, 15.6809, 15.6047, 15.6047, 15.5528,\n",
       "        15.4021, 13.8975, 13.8662, 13.5612, 13.3569, 13.3569, 13.1151, 12.9964,\n",
       "        12.9187, 12.6300, 12.5502, 12.3182, 12.0947, 12.0738, 12.0590, 12.0514,\n",
       "        11.9404, 11.9239, 11.8714, 11.8288, 11.7775, 11.5188, 11.4483, 11.4479,\n",
       "        11.4269, 11.3765, 11.3754, 11.3754, 11.0688, 11.0688, 10.9905, 10.9341,\n",
       "        10.9100, 10.9053, 10.8866, 10.8635, 10.8580, 10.7840, 10.7840, 10.7840,\n",
       "        10.7542, 10.7375, 10.7150, 10.7121, 10.7074, 10.6794, 10.6397, 10.6397,\n",
       "        10.6325, 10.6045, 10.5636, 10.5636, 10.5633, 10.4765, 10.4699, 10.4582,\n",
       "        10.4493, 10.4434, 10.4314, 10.3176, 10.1845, 10.1309, 10.1106, 10.0951,\n",
       "        10.0496, 10.0417, 10.0224,  9.9342,  9.9319,  9.9252,  9.8783,  9.8372,\n",
       "         9.8041,  9.7947,  9.7889,  9.7600,  9.7600,  9.7355,  9.7343,  9.7343,\n",
       "         9.7257,  9.6761,  9.6577,  9.6470,  9.5917,  9.5916,  9.5811,  9.5563,\n",
       "         9.5467,  9.5334,  9.5243,  9.4958]),\n",
       "indices=tensor([10036,  3322,  3341,  3328, 15708,  1300,  1432,  1290, 10721,  3070,\n",
       "        19152,  5031,  4971,  4988, 17551,  8634,  6635,  6498, 15840,  5405,\n",
       "         9503, 19954,  6553,  8520, 14495,  2605,   905, 12432, 15675,    29,\n",
       "         5465, 10700,  3375,  7941,  7764,  7863, 14835, 14888,  5052, 17042,\n",
       "           52, 20904,  4899, 20103, 13169,  3387,  3271,  3272, 15737, 15700,\n",
       "        11352, 17826,  7794,  5567, 12314, 12336, 19377, 10170,  7066,  6946,\n",
       "         7844,  4741,  3389, 19478,  3334,  5388,  4495,  4866,  5332, 17192,\n",
       "        18473,  2214, 15044,  5144,  7798,   272,  7785,  5235,  6659,  7842,\n",
       "        16291,  4496, 16164, 12263, 12265,  5177,  2263,  2171,  9516,   391,\n",
       "        12347, 20613, 14221,  5090,  6537,  8552, 20545, 16318,   994, 19942]))"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(torch.Tensor(chi2_stats), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['chr7.29631577', 'chr2.145686126', 'chr2.145743050',\n",
       "       'chr2.144811756', 'chr13.46775590', 'chr1.196721088',\n",
       "       'chr1.196721085', 'chr1.196721089', 'chr7.110434734',\n",
       "       'chr2.108960762', 'chr17.22777658', 'chr3.104950986',\n",
       "       'chr3.104990952', 'chr3.104991105', 'chr15.14899164',\n",
       "       'chr6.39478341', 'chr4.119528150', 'chr4.118967275',\n",
       "       'chr13.46784040', 'chr3.143256926', 'chr6.130924586',\n",
       "       'chr18.13497419', 'chr4.119363345', 'chr6.6674844',\n",
       "       'chr11.33028547', 'chr2.43248723', 'chr1.132793881',\n",
       "       'chr9.40597849', 'chr12.49835217', 'chr1.19942050',\n",
       "       'chr3.142909318', 'chr7.110496699', 'chr2.143772703',\n",
       "       'chr5.99313779', 'chr5.96661534', 'chr5.96661537',\n",
       "       'chr11.75034553', 'chr11.75034575', 'chr3.103333129',\n",
       "       'chr14.72625273', 'chr1.19942048', 'chr20.5368892',\n",
       "       'chr3.68664801', 'chr18.26857012', 'chr9.116352923',\n",
       "       'chr2.143817940', 'chr2.143822315', 'chr2.143821170',\n",
       "       'chr13.46746378', 'chr13.46818894', 'chr8.51523137',\n",
       "       'chr15.36249080', 'chr5.97384376', 'chr3.143212486',\n",
       "       'chr9.36755184', 'chr9.36755134', 'chr17.36926663',\n",
       "       'chr7.67214102', 'chr4.171707733', 'chr4.171707641',\n",
       "       'chr5.96905834', 'chr3.68649676', 'chr2.145317036',\n",
       "       'chr17.22920424', 'chr2.144733105', 'chr3.145044482',\n",
       "       'chr3.16365216', 'chr3.68749082', 'chr3.139665185',\n",
       "       'chr14.72619764', 'chr16.2706085', 'chr2.6203968',\n",
       "       'chr11.74622783', 'chr3.104537571', 'chr5.95887867',\n",
       "       'chr1.53168329', 'chr5.97285180', 'chr3.139559088',\n",
       "       'chr4.118769841', 'chr5.97382427', 'chr13.77646381',\n",
       "       'chr3.12148776', 'chr13.78064083', 'chr9.39811771',\n",
       "       'chr9.39811755', 'chr3.124167777', 'chr2.7694369', 'chr2.7694461',\n",
       "       'chr6.131088208', 'chr1.52440739', 'chr9.36862967',\n",
       "       'chr18.84669450', 'chr10.102894506', 'chr3.103010344',\n",
       "       'chr4.119044499', 'chr6.4744373', 'chr18.84663583',\n",
       "       'chr13.77798834', 'chr1.134526662', 'chr17.88547268'], dtype='<U15')"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SNP_names[np.array(torch.topk(torch.Tensor(chi2_stats), 100)[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 1., 0.],\n",
       "       [0., 0., 0., ..., 2., 2., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 1., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 1., 0.]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi2_stats, p_vals = chi2(X_r, Y)\n",
    "chi2_stats, p_vals\n",
    "X_r.values[:, np.array(torch.topk(torch.Tensor(chi2_stats), 100)[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 1., 0.],\n",
       "       [0., 0., 0., ..., 2., 2., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 1., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 1., 0.]])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.44697 (0.07058)\n",
      "\n",
      "Confusion matrix:\n",
      "[[144  54  19]\n",
      " [100  95  12]\n",
      " [ 40  44  26]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.66      0.57       217\n",
      "           1       0.49      0.46      0.47       207\n",
      "           2       0.46      0.24      0.31       110\n",
      "\n",
      "    accuracy                           0.50       534\n",
      "   macro avg       0.49      0.45      0.45       534\n",
      "weighted avg       0.49      0.50      0.48       534\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x1296 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def mlmodel(X, Y):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X.astype(float), Y, test_size=0.3, random_state=7)\n",
    "    model = SGDClassifier(loss='hinge')\n",
    "    # model = RandomForestClassifier()\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "    \n",
    "    kfold = KFold(n_splits=15)\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
    "    accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
    "    print(f\"Accuracy: {cv_results.mean():.5f} ({cv_results.std():.5f})\\n\"\n",
    "          + f\"\\nConfusion matrix:\\n{confusion_matrix(Y_test, Y_pred)}\\n\"\n",
    "          + f\"\\nClassification report:\\n{classification_report(Y_test, Y_pred)}\")\n",
    "    \n",
    "    fig, ax = plt.subplots(2, 1, figsize=(14, 2*9))\n",
    "    ax[0].hist(Y_pred.astype(int), color='#1f77b4', label=\"Y_pred\")\n",
    "    ax[1].hist(Y_test.astype(int), color='cyan', label=\"True\")\n",
    "    \n",
    "    for a in ax:\n",
    "        a.set(xlabel=\"Phenotype (Y)\")\n",
    "        a.legend()\n",
    "    plt.show()\n",
    "    \n",
    "mlmodel(X_dummy, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References & Further Reading:\n",
    "\n",
    "- Jie Yuan1:26 PM https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/\n",
    "- Jie Yuan1:30 PM https://stackoverflow.com/questions/25122999/scikit-learn-how-to-check-coefficients-significance\n",
    "- McKelvey & Zavoina https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds_env)",
   "language": "python",
   "name": "ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
