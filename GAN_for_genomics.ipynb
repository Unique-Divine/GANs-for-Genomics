{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PyTorch\n",
    "import torch\n",
    "\n",
    "# standard DS stack\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import pandas as pd\n",
    "# embed static images in the ipynb\n",
    "%matplotlib inline \n",
    "\n",
    "# neural network package\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "# computer vision\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# dataset loading\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# convenient package for plotting loss functions\n",
    "# from livelossplot import PlotLosses\n",
    "\n",
    "import copy\n",
    "import importlib.util # to run outside module\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.get_dataset_names()\n",
    "\n",
    "titanic_data = sns.load_dataset('titanic')\n",
    "\n",
    "def one_hot_encode(X, cat_features):\n",
    "    \"\"\"Converts an unstructued 2-D array consisting of categorical values\n",
    "    into a 2-D array consisting of one-hot-encoded vectors.\n",
    "\n",
    "    \n",
    "        X (pd.DataFrame): \n",
    "        cat_features (list): categorical feature names\n",
    "    \"\"\" \n",
    "    \n",
    "    X_cat = X[cat_features]\n",
    "    for cat in cat_features[:]:\n",
    "        X = X.drop(cat, axis=1)\n",
    "\n",
    "    # Replace the nonnumerical columns with one-hot encoded ones.\n",
    "    for name in cat_features[:]:\n",
    "        hot_one = pd.get_dummies(X_cat[name], prefix=name)\n",
    "        X = pd.concat([X, hot_one.set_index(X.index)], axis=1)\n",
    "    return X\n",
    "\n",
    "def preprocess_titanic(df=titanic_data):\n",
    "    df['sex'] = (df['sex'].values.astype(str) == 'male').astype(int)\n",
    "    df['alone'] = df['alone'].values.astype(int)\n",
    "    df.drop(['deck', 'alive'], axis=1,  inplace=True)\n",
    "    df = one_hot_encode(df, \n",
    "        ['who', 'embark_town', 'embarked', 'adult_male', 'class'])\n",
    "    df.dropna(inplace=True)\n",
    "    df = df.astype(float)\n",
    "    A = df.values\n",
    "    X, Y = A[:, 1:], A[:, 0].reshape(-1,1) \n",
    "    return X, Y\n",
    "    \n",
    "X, Y = preprocess_titanic()\n",
    "\n",
    "# Perform train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.3, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0076], grad_fn=<LeakyReluBackward0>)\n",
      "tensor([-0.0026], grad_fn=<LeakyReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# GPU for cloud training\n",
    "if torch.cuda.is_available(): \n",
    "    device = torch.device(\"cuda\") # device = GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # device = CPU\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, H_0)\n",
    "        self.fc2 = nn.Linear(H_0, H_1)\n",
    "        self.fc3 = nn.Linear(H_1, D_out)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        z = F.leaky_relu(self.fc1(z))\n",
    "        z = F.leaky_relu(self.fc2(z)) \n",
    "        z = F.leaky_relu(self.fc3(z))\n",
    "        return z\n",
    "\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, H_0)\n",
    "        self.fc2 = nn.Linear(H_0, H_1)\n",
    "        self.fc3 = nn.Linear(H_1, D_out)\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "#          x = self.dropout()\n",
    "        x = F.leaky_relu(self.fc2(x)) \n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "class Toy_Dataset(Dataset): # inherit from torch's Dataset class.\n",
    "    def __init__(self, train):\n",
    "        # data loading\n",
    "        if train == True:\n",
    "            self.X = torch.from_numpy(X_train.astype(np.float32))\n",
    "            self.Y = torch.from_numpy(Y_train.astype(np.float32))\n",
    "        else:\n",
    "            self.X = torch.from_numpy(X_test.astype(np.float32))\n",
    "            self.Y = torch.from_numpy(Y_test.astype(np.float32))\n",
    "\n",
    "        if self.X.shape[0] == self.Y.shape[0]:\n",
    "            self.n_samples = self.X.shape[0]\n",
    "        else:\n",
    "            raise ValueError(\"Shape mismatch\")\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "        # len(dataset)\n",
    "\n",
    "# Initialize constants\n",
    "n_features = X_train.shape[1]\n",
    "BATCH_SIZE, D_in, D_out = 15, n_features, 1\n",
    "H_0, H_1 = int(0.7*n_features), int(0.4*n_features)\n",
    "\n",
    "# Set DataLoaders    \n",
    "train_set = Toy_Dataset(train=True)\n",
    "test_set = Toy_Dataset(train=False)\n",
    "train_dl = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl = DataLoader(dataset=test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Initialize networks\n",
    "G = Generator()\n",
    "D = Discriminator()\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def check_models():\n",
    "    x = torch.from_numpy(X_train[0]).float() \n",
    "    print( G(x) )\n",
    "    print( D(x) )\n",
    "    \n",
    "check_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "A sequential container.\n",
       "Modules will be added to it in the order they are passed in the constructor.\n",
       "Alternatively, an ordered dict of modules can also be passed in.\n",
       "\n",
       "To make it easier to understand, here is a small example::\n",
       "\n",
       "    # Example of using Sequential\n",
       "    model = nn.Sequential(\n",
       "              nn.Conv2d(1,20,5),\n",
       "              nn.ReLU(),\n",
       "              nn.Conv2d(20,64,5),\n",
       "              nn.ReLU()\n",
       "            )\n",
       "\n",
       "    # Example of using Sequential with OrderedDict\n",
       "    model = nn.Sequential(OrderedDict([\n",
       "              ('conv1', nn.Conv2d(1,20,5)),\n",
       "              ('relu1', nn.ReLU()),\n",
       "              ('conv2', nn.Conv2d(20,64,5)),\n",
       "              ('relu2', nn.ReLU())\n",
       "            ]))\n",
       "\u001b[1;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\uniqu\\anaconda3\\envs\\ds_env\\lib\\site-packages\\torch\\nn\\modules\\container.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     ConvReLU2d, ConvReLU3d, LinearReLU, ConvBn2d, ConvBnReLU2d, _Transition, ConvBNReLU, DeepLabHead, ASPPConv, ASPPPooling, ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.Sequential?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Applies the element-wise function:\n",
       "\n",
       ".. math::\n",
       "    \\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)} {\\exp(x) + \\exp(-x)}\n",
       "\n",
       "Shape:\n",
       "    - Input: :math:`(N, *)` where `*` means, any number of additional\n",
       "      dimensions\n",
       "    - Output: :math:`(N, *)`, same shape as the input\n",
       "\n",
       ".. image:: scripts/activation_images/Tanh.png\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> m = nn.Tanh()\n",
       "    >>> input = torch.randn(2)\n",
       "    >>> output = m(input)\n",
       "\u001b[1;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\uniqu\\anaconda3\\envs\\ds_env\\lib\\site-packages\\torch\\nn\\modules\\activation.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.Tanh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Conceptual References:\n",
    "- [*GANS*, Google Developers](https://developers.google.com/machine-learning/gan/gan_structure)\n",
    "- [Ian Goodfellow on Lex Fridman podcast](https://www.youtube.com/watch?v=Z6rxFNMGdn0&t=2826s&ab_channel=LexFridman)\n",
    "- [Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2014). Generative adversarial nets. In *Advances in neural information processing systems* (pp. 2672-2680).](https://arxiv.org/pdf/1406.2661.pdf)\n",
    "- [Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., & Chen, X. (2016). Improved techniques for training gans. In *Advances in neural information processing systems* (pp. 2234-2242).](https://arxiv.org/pdf/1606.03498.pdf)\n",
    "\n",
    "Implementation References:\n",
    "- [eriklindernoren/PyTorch-GAN](https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py)\n",
    "  - [list of successful GAN architectures](https://github.com/eriklindernoren/PyTorch-GAN#gan)\n",
    "- [fast ai, free GPU w/ Google Colab guide](https://www.kdnuggets.com/2018/02/fast-ai-lesson-1-google-colab-free-gpu.html)\n",
    "- [PyTorch GAN, github.com/devnag](https://github.com/devnag/pytorch-generative-adversarial-networks/blob/master/gan_pytorch.py)\n",
    "- [Brownlee, Jason (2020). How to Code the GAN Training Algorithm. *machinelearningmastery.com*](https://machinelearningmastery.com/how-to-code-the-generative-adversarial-network-training-algorithm-and-loss-functions/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds_env)",
   "language": "python",
   "name": "ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
